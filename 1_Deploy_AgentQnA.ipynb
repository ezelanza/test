{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why to Build an Agent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† From Talking to Doing: Why We Need More Than Just LLMs\n",
    "\n",
    "Imagine you ask a **consultant**:\n",
    "\n",
    "> ‚ÄúCan you help me plan my trip to Paris?‚Äù\n",
    "\n",
    "They reply:\n",
    "\n",
    "> ‚ÄúSure! You should book a flight, find a hotel near the Eiffel Tower, and maybe buy museum tickets in advance.‚Äù\n",
    "\n",
    "Helpful ‚Äî but **you‚Äôre still doing all the work**.\n",
    "\n",
    "That‚Äôs what a **plain LLM** does: it provides good advice, but doesn‚Äôt take action.\n",
    "\n",
    "---\n",
    "\n",
    "Now imagine asking a **personal assistant** the same question:\n",
    "\n",
    "> ‚ÄúCan you plan my trip to Paris?‚Äù\n",
    "\n",
    "They respond:\n",
    "\n",
    "> ‚úÖ Flight booked  \n",
    "> ‚úÖ Hotel reserved  \n",
    "> ‚úÖ Museum tickets purchased  \n",
    "> ‚úÖ Itinerary sent to your inbox\n",
    "\n",
    "That‚Äôs what an **agent** does: it combines language understanding with **tools, APIs, and logic** to get real-world tasks done.\n",
    "\n",
    "---\n",
    "\n",
    "**üîÅ TL;DR**  \n",
    "- üßë‚Äçüè´ **LLM = Smart talker (consultant)**  \n",
    "- üßë‚Äçüíº **Agent = Smart doer (assistant with tools)**\n",
    "\n",
    "---\n",
    "\n",
    "### **LLMs vs RAG vs Agents**\n",
    "\n",
    "| Layer       | Description                                 | Limitation                             |\n",
    "|-------------|---------------------------------------------|----------------------------------------|\n",
    "| **LLMs**    | General-purpose language models             | Can‚Äôt access real-time data or act     |\n",
    "| **RAG**     | Adds external context from knowledge sources| Still limited to generating text       |\n",
    "| **Agents**  | Use tools, APIs, and workflows to act       | Built to *do*, not just *say*          |\n",
    "\n",
    "---\n",
    "\n",
    "To handle real-world tasks, we need systems that can:\n",
    "\n",
    "- üîó **Connect to external sources** (APIs, live data, tools)  \n",
    "- ‚öôÔ∏è **Trigger actions** (bookings, queries, automation)  \n",
    "- üßë‚Äçüî¨ **Specialize** in specific domains (e.g., legal, finance)\n",
    "\n",
    "**AI Agents** combine reasoning with action ‚Äî turning intent into outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is an Agent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents are autonomous systems that perceive their environment, reason, and execute actions to achieve specific goals. They can interact with users, retrieve and process data, and execute tasks based on rules or learning models.\n",
    "\n",
    " Key Features:\n",
    "- ‚úÖ Autonomous ‚Äì Operate with minimal human intervention\n",
    "- ‚úÖ Perceptual ‚Äì Analyze information from text, voice, images, or structured data\n",
    "- ‚úÖ Goal-oriented ‚Äì Work to accomplish tasks such as answering questions or summarizing content\n",
    "- ‚úÖ Interactive ‚Äì Communicate with humans or with other agents\n",
    "- ‚úÖ Adaptive ‚Äì Improve over time through learning or feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Types of Agents**  \n",
    "Agents can be classified based on their reasoning and decision-making capabilities:\n",
    "\n",
    "| Type | Description | Example |\n",
    "|------|-------------|---------|\n",
    "| **Reactive Agents** | Respond only to current inputs, without memory | Chatbot answering direct questions |\n",
    "| **Goal-Based Agents** | Use goals to guide their decisions | Recommendation systems |\n",
    "| **Utility-Based Agents** | Optimize for the best possible decision | Self-driving cars adjusting speed |\n",
    "| **Learning Agents** | Improve with data and experience | AI models fine-tuned for specific tasks |\n",
    "\n",
    "### **Example: Difference Between Reactive and Goal-Based Agents**  \n",
    "üöó **Reactive Agent:** A self-driving car stops when the traffic light is red but doesn‚Äôt plan ahead.  \n",
    "üõ£Ô∏è **Goal-Based Agent:** The car not only stops but also adjusts its speed to optimize fuel efficiency.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of an Agent (TO BE COMPLETED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AgentType, initialize_agent\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tool\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-IntelCorporation/Documents/GitHub repos/.venv/lib/python3.13/site-packages/langchain/chat_models/__init__.py:29\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m chat_models\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# If not in interactive env, raise warning.\u001b[39;00m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_interactive_env():\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.tools import Tool\n",
    "import os\n",
    "\n",
    "# Create a function for mathematical calculations\n",
    "def calculate(input_str):\n",
    "    \"\"\"Evaluates simple mathematical expressions\"\"\"\n",
    "    try:\n",
    "        return str(eval(input_str))\n",
    "    except Exception as e:\n",
    "        return f\"Calculation error: {e}\"\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "# Use the Tool method\n",
    "Math_tool = Tool(\n",
    "    name=\"Calculator\",\n",
    "    func=calculate,\n",
    "    description=\"Useful ONLY for calculating simple math expressions. Expected input: '2+2'\"\n",
    ")\n",
    "# You can try removing the word ONLY and see the difference.\n",
    "# The model only has one tool and will try to use it.\n",
    "# When an agent has limited tools, it may try to use whatever it has‚Äîeven if it's not appropriate.\n",
    "\n",
    "# Initialize the agent with additional tools\n",
    "agent = initialize_agent(\n",
    "    tools=[Math_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test the agent\n",
    "print(agent.run(\"What is the capital of France?\"))   # This is a non-math question\n",
    "print(agent.run(\"What is 12 * 8?\"))                  # This is a math question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† From Language to Action: The Challenge of Real AI Agents\n",
    "\n",
    "Large Language Models (LLMs) are excellent at generating natural language ‚Äî but real-world applications demand more than just fluent responses. To truly assist users, AI systems must:\n",
    "\n",
    "- Use tools and APIs to take action\n",
    "- Retain and reason across multiple steps\n",
    "- Query databases or retrieve relevant documents\n",
    "- Operate reliably in production environments\n",
    "\n",
    "Building these capabilities from scratch is complex. Most frameworks either oversimplify agent logic or lock you into rigid architectures that are hard to scale or customize.\n",
    "\n",
    "---\n",
    "\n",
    "### What Is OPEA?\n",
    "\n",
    "**OPEA (Open Platform for Enterprise Agents)** is an open source framework designed to simplify the development and deployment of production-ready GenAI deployments.\n",
    "\n",
    "It gives you the scaffolding to go beyond chat and build systems that can reason, act, and specialize.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use OPEA?\n",
    "\n",
    "| Feature                      | What It Enables                                |\n",
    "|-----------------------------|-------------------------------------------------|\n",
    "| **Agentic Blueprints**       | Skip boilerplate ‚Äî use prebuilt, customizable agent workflows (RAG, tool use, multi-agent) |\n",
    "| **Modular by Design**        | Swap in any LLM, memory backend, or external tool |\n",
    "| **Deployment-Ready**         | Includes Docker, APIs, tracing, and monitoring |\n",
    "| **Enterprise-Friendly**      | Secure, observable, and compatible with on-prem or cloud deployments |\n",
    "| **Community-Driven**         | Actively maintained by contributors from Intel and the open-source ecosystem |\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Built for Real Applications\n",
    "\n",
    "OPEA bridges the gap between research prototypes and real-world deployments:\n",
    "\n",
    "- Connects to vector databases like Chroma, Qdrant, Weaviate\n",
    "- Supports memory and stateful agent workflows\n",
    "- Enables tool calling via APIs, SQL, or function schemas\n",
    "- Provides observability and debugging for agent flows\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy OPEA AgentQnA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervisor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervisor uses REACT methodology which is : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thought ‚Üí Action ‚Üí Observation ‚Üí Thought ‚Üí ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPEA is built with a supervisor logic \n",
    "\n",
    "`GenAIComps/comps/agent/src/integrations/strategy/react/prompt.py`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt example for the agent :\n",
    "```\n",
    "REACT_SYS_MESSAGE = \"\"\"\\\n",
    "Decompose the user request into a series of simple tasks when necessary and solve the problem step by step.\n",
    "When you cannot get the answer at first, do not give up. Reflect on the info you have from the tools and try to solve the problem in a different way.\n",
    "Please follow these guidelines when formulating your answer:\n",
    "1. If the question contains a false premise or assumption, answer ‚Äúinvalid question‚Äù.\n",
    "2. If you are uncertain or do not know the answer, respond with ‚ÄúI don‚Äôt know‚Äù.\n",
    "3. Give concise, factual and relevant answers.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Steps are:\n",
    "- Create the prompt\n",
    "- Call the llm, `llm.invoke`\n",
    "\n",
    "\n",
    "Build the tool set the agent have available, in this case \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy in Kubernetes\n",
    "If you don't have your EKS cluster you can follow this guide to deploy your Kubernetes Cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/opea-project/GenAIExamples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export WORKDIR=$(pwd) && sudo mkdir -p \"$WORKDIR/mnt/tools\" && sudo chmod 777 \"$WORKDIR/mnt/tools\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Agents configurations\n",
    "\n",
    "WORKDIR = !pwd\n",
    "WORKDIR = WORKDIR[0]\n",
    "\n",
    "# tools used by supervisor\n",
    "!wget https://raw.githubusercontent.com/opea-project/GenAIExamples/refs/heads/main/AgentQnA/tools/supervisor_agent_tools.yaml -O {WORKDIR}/mnt/tools/supervisor_agent_tools.yaml\n",
    "!wget https://raw.githubusercontent.com/opea-project/GenAIExamples/refs/heads/main/AgentQnA/tools/tools.py -O {WORKDIR}/mnt/tools/tools.py\n",
    "!wget https://raw.githubusercontent.com/opea-project/GenAIExamples/refs/heads/main/AgentQnA/tools/pycragapi.py -O {WORKDIR}/mnt/tools/pycragapi.py\n",
    "\n",
    "# tools used by rag agent\n",
    "!wget https://raw.githubusercontent.com/opea-project/GenAIExamples/refs/heads/main/AgentQnA/tools/worker_agent_tools.yaml -O {WORKDIR}/mnt/tools/worker_agent_tools.yaml\n",
    "!wget https://raw.githubusercontent.com/opea-project/GenAIExamples/refs/heads/main/AgentQnA/tools/worker_agent_tools.py -O {WORKDIR}/mnt/tools/worker_agent_tools.py\n",
    "\n",
    "#Download the sqlite database binary file\n",
    "!wget https://raw.githubusercontent.com/lerocha/chinook-database/refs/heads/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite -O {WORKDIR}/mnt/tools/Chinook_Sqlite.sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Agents tools and configuration to control-plane\n",
    "\n",
    "import os\n",
    "\n",
    "# Define local tools directory\n",
    "WORKDIR = os.getcwd()\n",
    "TOOLS_DIR = os.path.join(WORKDIR, \"mnt/tools\")\n",
    "\n",
    "# Create destination folder in the kind node\n",
    "!docker exec kind-control-plane mkdir -p /mnt/tools\n",
    "\n",
    "# Copy everything inside mnt/tools/ into kind-control-plane:/mnt/tools/\n",
    "!docker cp {TOOLS_DIR}/. kind-control-plane:/mnt/tools/\n",
    "\n",
    "print(\"‚úÖ All tools copied into kind-control-plane:/mnt/tools/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naviagate to your variant configuration\n",
    "\n",
    "![image](./Images/variant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify all agents to look as follows:\n",
    "```bash \n",
    "ragagent:\n",
    "    model: \"gpt-4o-mini-2024-07-18\"\n",
    "    llm_engine: openai\n",
    "    OPENAI_API_KEY: \"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\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy helm chart using OpenAI\n",
    "!helm upgrade --install agentqna agentqna \\\n",
    "  -f agentqna/variant_openai-values.yaml\\\n",
    "  --set global.HUGGINGFACEHUB_API_TOKEN=\"your_hf_key\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for pod status . ALL CONTAINERS MUST BE ON RUNNING STATE (1/1) \n",
    "# It can take ~ 10 minutes\n",
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s http://localhost:9090/v1/chat/completions \\\n",
    "  -X POST \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"messages\": \"2+2\"}' | jq -r '.text'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest documents for RAG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.Popen([\"kubectl\", \"port-forward\", \"svc/agentqna-data-prep\", \"6007:6007\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export WORKDIR=$(pwd) && \\\n",
    "export host_ip=localhost && \\\n",
    "cd GenAIExamples/AgentQnA/retrieval_tool/ && \\\n",
    "bash run_ingest_data.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate info submitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This information will be used for the rag agent\n",
    "!cat GenAIExamples/AgentQnA/example_data/test_docs_music.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curl basic question, and check logs\n",
    "\n",
    "Curl question based on RAG and check logs\n",
    "\n",
    "Curl SQL based question check logs "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
